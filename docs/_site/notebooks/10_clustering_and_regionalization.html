<!DOCTYPE html>
<html lang="en">
  

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width,minimum-scale=1">

  <title>Clustering &amp; Regionalization</title>
  <meta name="description" content="Clustering and Regionalization">

  <link rel="canonical" href="/book/book/notebooks/10_clustering_and_regionalization.html">
  <link rel="alternate" type="application/rss+xml" title="Geographic Data Science with Python" href="/book/book/feed.xml">

  <meta property="og:url"         content="/book/book/notebooks/10_clustering_and_regionalization.html" />
<meta property="og:type"        content="article" />
<meta property="og:title"       content="Clustering &amp; Regionalization" />
<meta property="og:description" content="Clustering and Regionalization" />
<meta property="og:image"       content="" />


  <script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "NewsArticle",
  "mainEntityOfPage":
    "/book/notebooks/10_clustering_and_regionalization.html",
  "headline":
    "Clustering &amp; Regionalization",
  "datePublished":
    "2019-04-26T23:33:35+00:00",
  "dateModified":
    "2019-04-26T23:33:35+00:00",
  "description":
    "Clustering and Regionalization",
  "author": {
    "@type": "Person",
    "name": "Sergio J. Rey, Dani Arribas-Bel, Levi J. Wolf"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Data 100 at UC Berkeley",
    "logo": {
      "@type": "ImageObject",
      "url": "/book",
      "width": 60,
      "height": 60
    }
  },
  "image": {
    "@type": "ImageObject",
    "url": "/book",
    "height": 60,
    "width": 60
  }
}

  </script>
  <link rel="stylesheet" href="/book/assets/css/styles.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css ">
  <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">

  <!-- <link rel="manifest" href="/manifest.json"> -->
  <!-- <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#efae0a"> -->
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/mstile-144x144.png">
  <meta name="theme-color" content="#233947">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="/book/images/logo/favicon.ico">

  <!-- MathJax Config -->
  <!-- Allow inline math using $ and automatically break long math lines -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: true,
        processEnvironments: true
    },
    CommonHTML: {
        linebreaks: {
            automatic: true,
        },
    },
});
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML' async></script>

  <!-- DOM updating function -->
  <script>
const runWhenDOMLoaded = cb => {
  if (document.readyState != 'loading') {
    cb()
  } else if (document.addEventListener) {
    document.addEventListener('DOMContentLoaded', cb)
  } else {
    document.attachEvent('onreadystatechange', function() {
      if (document.readyState == 'complete') cb()
    })
  }
}

// Helper function to init things quickly
initFunction = function(myfunc) {
  runWhenDOMLoaded(myfunc);
  document.addEventListener('turbolinks:load', myfunc);
};
</script>

  <!-- Define some javascript variables that will be useful in other javascript -->
  <script>
    const site_basename = '/book';
  </script>

  <!-- Add AnchorJS to let headers be linked -->
  <script src="/book/assets/js/anchor.min.js"  type="text/javascript"></script>
  <script>

initFunction(function () {
    anchors.add("main h1, main h2, main h3, main h4")
});

</script>

  <!-- Include Turbolinks to make page loads fast -->
  <!-- https://github.com/turbolinks/turbolinks -->
  <script src="/book/assets/js/turbolinks.js" async></script>
  <meta name="turbolinks-cache-control" content="no-cache">

  <!-- Load nbinteract for widgets -->
  

  <!-- Load Thebelab for interactive widgets -->
  <!-- Include Thebelab for interactive code if it's enabled -->


<!-- Display Thebelab button in each code cell -->
<script>
/**
 * Set up thebelab button for code blocks
 */

const thebelabCellButton = id =>
  `<a id="thebelab-cell-button-${id}" class="btn thebebtn o-tooltip--left" data-tooltip="Interactive Mode">
    <img src="/book/assets/images/edit-button.svg" alt="Start interactive mode">
  </a>`


const addThebelabButtonToCodeCells =  () => {

  const codeCells = document.querySelectorAll('div.input_area > div.highlighter-rouge:not(.output) pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("thebelab-cell-button-" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', thebelabCellButton(id));
    }
  })
}

initFunction(addThebelabButtonToCodeCells);
</script>



<script type="text/x-thebe-config">
    {
      requestKernel: true,
      binderOptions: {
        repo: 'gdsbook/geographic-data-science',
        ref: 'master',
      },
      codeMirrorConfig: {
        theme: ""
      },
      kernelOptions: {
        name: 'python3',
      }
    }
</script>
<script src="https://unpkg.com/thebelab@0.4.0/lib/index.js"></script>
<script>
    /**
     * Add attributes to Thebelab blocks
     */

    const initThebelab = () => {
        const addThebelabToCodeCells = () => {
            console.log("Adding thebelab to code cells...");
            // If Thebelab hasn't loaded, wait a bit and try again. This
            // happens because we load ClipboardJS asynchronously.
            if (window.thebelab === undefined) {
                setTimeout(addThebelabToCodeCells, 250)
            return
            }

            // If we already detect a Thebelab cell, don't re-run
            if (document.querySelectorAll('div.thebelab-cell').length > 0) {
                return;
            }

            // Find all code cells, replace with Thebelab interactive code cells
            const codeCells = document.querySelectorAll('.input_area pre')
            codeCells.forEach((codeCell, index) => {
                const id = codeCellId(index)
                codeCell.setAttribute('data-executable', 'true')

                // Figure out the language it uses and add this too
                var parentDiv = codeCell.parentElement.parentElement;
                var arrayLength = parentDiv.classList.length;
                for (var ii = 0; ii < arrayLength; ii++) {
                    var parts = parentDiv.classList[ii].split('language-');
                    if (parts.length === 2) {
                        // If found, assign dataLanguage and break the loop
                        var dataLanguage = parts[1];
                        break;
                    }
                }
                codeCell.setAttribute('data-language', dataLanguage)

                // If the code cell is hidden, show it
                var inputCheckbox = document.querySelector(`input#hidebtn${codeCell.id}`);
                if (inputCheckbox !== null) {
                    setCodeCellVisibility(inputCheckbox, 'visible');
                }
            });

            // Remove the event listener from the page so keyboard press doesn't
            // Change page
            document.removeEventListener('keydown', initPageNav)
            keyboardListener = false;

            // Init thebelab
            thebelab.bootstrap();

            // Remove copy buttons since they won't work anymore
            const copyAndThebeButtons = document.querySelectorAll('.copybtn, .thebebtn')
            copyAndThebeButtons.forEach((button, index) => {
                button.remove();
            });

            // Remove outputs since they'll be stale
            const outputs = document.querySelectorAll('.output *, .output')
            outputs.forEach((output, index) => {
                output.remove();
            });
        }

        // Add event listener for the function to modify code cells
        const thebelabButtons = document.querySelectorAll('[id^=thebelab], [id$=thebelab]')
        thebelabButtons.forEach((thebelabButton,index) => {
            if (thebelabButton === null) {
                setTimeout(initThebelab, 250)
                return
            };
            thebelabButton.addEventListener('click', addThebelabToCodeCells);
        });
    }

    // Initialize Thebelab
    initFunction(initThebelab);
</script>



  <!-- Load the auto-generating TOC -->
  <script src="/book/assets/js/tocbot.min.js"  type="text/javascript"></script>
  <script>
var initToc = function () {
  tocbot.init({
    tocSelector: 'nav.onthispage',
    contentSelector: '.c-textbook__content',
    headingSelector: 'h2, h3',
    orderedList: false,
    collapseDepth: 6,
    listClass: 'toc__menu',
    activeListItemClass: "",  // Not using
    activeLinkClass: "", // Not using
  });
  tocbot.refresh();
}
initFunction(initToc);
</script>

  <!-- Google analytics -->
  <script src="/book/assets/js/ga.js" async></script>

  <!-- Clipboard copy button -->
  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" async></script>

  <!-- Load JS that depends on site variables -->
  <script>
/**
 * Set up copy/paste for code blocks
 */
const codeCellId = index => `codecell${index}`

const clipboardButton = id =>
  `<a id="copy-button-${id}" class="btn copybtn o-tooltip--left" data-tooltip="Copy" data-clipboard-target="#${id}">
    <img src="/book/assets/images/copy-button.svg" alt="Copy to clipboard">
  </a>`

// Clears selected text since ClipboardJS will select the text when copying
const clearSelection = () => {
  if (window.getSelection) {
    window.getSelection().removeAllRanges()
  } else if (document.selection) {
    document.selection.empty()
  }
}

// Changes tooltip text for two seconds, then changes it back
const temporarilyChangeTooltip = (el, newText) => {
  const oldText = el.getAttribute('data-tooltip')
  el.setAttribute('data-tooltip', newText)
  setTimeout(() => el.setAttribute('data-tooltip', oldText), 2000)
}

const addCopyButtonToCodeCells = () => {
  // If ClipboardJS hasn't loaded, wait a bit and try again. This
  // happens because we load ClipboardJS asynchronously.
  if (window.ClipboardJS === undefined) {
    setTimeout(addCopyButtonToCodeCells, 250)
    return
  }

  const codeCells = document.querySelectorAll('div.c-textbook__content > div.highlighter-rouge > div.highlight > pre, div.input_area pre')
  codeCells.forEach((codeCell, index) => {
    const id = codeCellId(index)
    codeCell.setAttribute('id', id)
    if (document.getElementById("copy-button" + id) == null) {
      codeCell.insertAdjacentHTML('afterend', clipboardButton(id));
    }
  })

  const clipboard = new ClipboardJS('.copybtn')
  clipboard.on('success', event => {
    clearSelection()
    temporarilyChangeTooltip(event.trigger, 'Copied!')
  })

  clipboard.on('error', event => {
    temporarilyChangeTooltip(event.trigger, 'Failed to copy')
  })

  // Get rid of clipboard before the next page visit to avoid memory leak
  document.addEventListener('turbolinks:before-visit', () =>
    clipboard.destroy()
  )
}

initFunction(addCopyButtonToCodeCells);
</script>


  <!-- Hide cell code -->
  
<script>
/**
Add buttons to hide code cells
*/


var setCodeCellVisibility = function(inputField, kind) {
    // Update the image and class for hidden
    var id = inputField.getAttribute('data-id');
    var codeCell = document.querySelector(`#${id} div.highlight`);

    if (kind === "visible") {
        codeCell.classList.remove('hidden');
        inputField.checked = true;
    } else {
        codeCell.classList.add('hidden');
        inputField.checked = false;
    }
}

var toggleCodeCellVisibility = function (event) {
    // The label is clicked, and now we decide what to do based on the input field's clicked status
    if (event.target.tagName === "LABEL") {
        var inputField = event.target.previousElementSibling;
    } else {
        // It is the span inside the target
        var inputField = event.target.parentElement.previousElementSibling;
    }

    if (inputField.checked === true) {
        setCodeCellVisibility(inputField, "visible");
    } else {
        setCodeCellVisibility(inputField, "hidden");
    }
}


// Button constructor
const hideCodeButton = id => `<input class="hidebtn" type="checkbox" id="hidebtn${id}" data-id="${id}"><label title="Toggle cell" for="hidebtn${id}" class="plusminus"><span class="pm_h"></span><span class="pm_v"></span></label>`

var addHideButton = function () {
  // If a hide button is already added, don't add another
  if (document.querySelector('div.hidecode input') !== null) {
      return;
  }

  // Find the input cells and add a hide button
  document.querySelectorAll('div.input_area').forEach(function (item, index) {
    if (!item.classList.contains("hidecode")) {
        // Skip the cell if it doesn't have a hidecode class
        return;
    }

    const id = codeCellId(index)
    item.setAttribute('id', id);
    // Insert the button just inside the end of the next div
    item.querySelector('div').insertAdjacentHTML('beforeend', hideCodeButton(id))

    // Set up the visibility toggle
    hideLink = document.querySelector(`#${id} div.highlight + input + label`);
    hideLink.addEventListener('click', toggleCodeCellVisibility)
  });
}


// Initialize the hide buttos
var initHiddenCells = function () {
    // Add hide buttons to the cells
    addHideButton();

    // Toggle the code cells that should be hidden
    document.querySelectorAll('div.hidecode input').forEach(function (item) {
        setCodeCellVisibility(item, 'hidden');
        item.checked = true;
    })
}

initFunction(initHiddenCells);

</script>


  <!-- Load custom website scripts -->
  <script src="/book/assets/js/scripts.js" async></script>

  <!-- Load custom user CSS and JS  -->
  <script src="/book/assets/custom/custom.js" async></script>
  <link rel="stylesheet" href="/book/assets/custom/custom.css">

  <!-- Update interact links w/ REST param, is defined in includes so we can use templates -->
  
<script>
/**
  * To auto-embed hub URLs in interact links if given in a RESTful fashion
 */

function getJsonFromUrl(url) {
  var query = url.split('?');
  if (query.length < 2) {
    // No queries so just return false
    return false;
  }
  query = query[1];
  // Collect REST params into a dictionary
  var result = {};
  query.split("&").forEach(function(part) {
    var item = part.split("=");
    result[item[0]] = decodeURIComponent(item[1]);
  });
  return result;
}
    
function dict2param(dict) {
    params = Object.keys(dict).map(function(k) {
        return encodeURIComponent(k) + '=' + encodeURIComponent(dict[k])
    });
    return params.join('&')
}

// Parse a Binder URL, converting it to the string needed for JupyterHub
function binder2Jupyterhub(url) {
  newUrl = {};
  parts = url.split('v2/gh/')[1];
  // Grab the base repo information
  repoinfo = parts.split('?')[0];
  var [org, repo, ref] = repoinfo.split('/');
  newUrl['repo'] = ['https://github.com', org, repo].join('/');
  newUrl['branch'] = ref
  // Grab extra parameters passed
  params = getJsonFromUrl(url);
  if (params['filepath'] !== undefined) {
    newUrl['subPath'] = params['filepath']
  }
  return dict2param(newUrl);
}

// Filter out potentially unsafe characters to prevent xss
function safeUrl(url)
{
   return String(encodeURIComponent(url))
            .replace(/&/g, '&amp;')
            .replace(/"/g, '&quot;')
            .replace(/'/g, '&#39;')
            .replace(/</g, '&lt;')
            .replace(/>/g, '&gt;');
}

function addParamToInternalLinks(hub) {
  var links = document.querySelectorAll("a").forEach(function(link) {
    var href = link.href;
    // If the link is an internal link...
    if (href.search("") !== -1 || href.startsWith('/') || href.search("127.0.0.1:") !== -1) {
      // Assume we're an internal link, add the hub param to it
      var params = getJsonFromUrl(href);
      if (params !== false) {
        // We have REST params, so append a new one
        params['jupyterhub'] = hub;
      } else {
        // Create the REST params
        params = {'jupyterhub': hub};
      }
      // Update the link
      var newHref = href.split('?')[0] + '?' + dict2param(params);
      link.setAttribute('href', decodeURIComponent(newHref));
    }
  });
  return false;
}


// Update interact links
function updateInteractLink() {
    // hack to make this work since it expects a ? in the URL
    rest = getJsonFromUrl("?" + location.search.substr(1));
    jupyterHubUrl = rest['jupyterhub'];
    var hubType = null;
    var hubUrl = null;
    if (jupyterHubUrl !== undefined) {
      hubType = 'jupyterhub';
      hubUrl = jupyterHubUrl;
    }

    if (hubType !== null) {
      // Sanitize the hubUrl
      hubUrl = safeUrl(hubUrl);

      // Add HTTP text if omitted
      if (hubUrl.indexOf('http') < 0) {hubUrl = 'http://' + hubUrl;}
      var interactButtons = document.querySelectorAll("button.interact-button")
      var lastButton = interactButtons[interactButtons.length-1];
      var link = lastButton.parentElement;

      // If we've already run this, skip the link updating
      if (link.nextElementSibling !== null) {
        return;
      }

      // Update the link and add context div
      var href = link.getAttribute('href');
      if (lastButton.id === 'interact-button-binder') {
        // If binder links exist, we need to re-work them for jupyterhub
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // If localhost, assume we're working from a local Jupyter server and remove `/hub`
          first = [hubUrl, 'git-sync'].join('/')
        } else {
          first = [hubUrl, 'hub', 'user-redirect', 'git-sync'].join('/')
        }
        href = first + '?' + binder2Jupyterhub(href);
      } else {
        // If interact button isn't binderhub, assume it's jupyterhub
        // If JupyterHub links, we only need to replace the hub url
        href = href.replace("", hubUrl);
        if (hubUrl.indexOf('http%3A%2F%2Flocalhost') > -1) {
          // Assume we're working from a local Jupyter server and remove `/hub`
          href = href.replace("/hub/user-redirect", "");
        }
      }
      link.setAttribute('href', decodeURIComponent(href));

      // Add text after interact link saying where we're launching
      hubUrlNoHttp = decodeURIComponent(hubUrl).replace('http://', '').replace('https://', '');
      link.insertAdjacentHTML('afterend', '<div class="interact-context">on ' + hubUrlNoHttp + '</div>');

      // Update internal links so we retain the hub url
      addParamToInternalLinks(hubUrl);
    }
}

runWhenDOMLoaded(updateInteractLink)
document.addEventListener('turbolinks:load', updateInteractLink)
</script>


  <!-- Lunr search code - will only be executed on the /search page -->
  <script src="/book/assets/js/lunr/lunr.min.js" type="text/javascript"></script>
  <script>var initQuery = function() {
  // See if we have a search box
  var searchInput = document.querySelector('input#lunr_search');
  if (searchInput === null) {
    return;
  }

  // Function to parse our lunr cache
  var idx = lunr(function () {
    this.field('title')
    this.field('excerpt')
    this.field('categories')
    this.field('tags')
    this.ref('id')

    this.pipeline.remove(lunr.trimmer)

    for (var item in store) {
      this.add({
        title: store[item].title,
        excerpt: store[item].excerpt,
        categories: store[item].categories,
        tags: store[item].tags,
        id: item
      })
    }
  });

  // Run search upon keyup
  searchInput.addEventListener('keyup', function () {
    var resultdiv = document.querySelector('#results');
    var query = document.querySelector("input#lunr_search").value.toLowerCase();
    var result =
      idx.query(function (q) {
        query.split(lunr.tokenizer.separator).forEach(function (term) {
          q.term(term, { boost: 100 })
          if(query.lastIndexOf(" ") != query.length-1){
            q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
          }
          if (term != ""){
            q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })
          }
        })
      });

      // Empty the results div
      while (resultdiv.firstChild) {
        resultdiv.removeChild(resultdiv.firstChild);
      }

    resultdiv.insertAdjacentHTML('afterbegin', '<p class="results__found">'+result.length+' Result(s) found</p>');
    for (var item in result) {
      var ref = result[item].ref;
      if(store[ref].teaser){
        var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<div class="archive__item-teaser">'+
                '<img src="'+store[ref].teaser+'" alt="">'+
              '</div>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      else{
    	  var searchitem =
          '<div class="list__item">'+
            '<article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">'+
              '<h2 class="archive__item-title" itemprop="headline">'+
                '<a href="'+store[ref].url+'" rel="permalink">'+store[ref].title+'</a>'+
              '</h2>'+
              '<p class="archive__item-excerpt" itemprop="description">'+store[ref].excerpt.split(" ").splice(0,20).join(" ")+'...</p>'+
            '</article>'+
          '</div>';
      }
      resultdiv.insertAdjacentHTML('beforeend', searchitem);
    }
  });
};

initFunction(initQuery);
</script>
</head>

  <body>
    <!-- .js-show-sidebar shows sidebar by default -->
    <div id="js-textbook" class="c-textbook js-show-sidebar">
      



<nav id="js-sidebar" class="c-textbook__sidebar">
  <a href="https://github.com/gdsbook/geographic-data-science"><img src="/book/images/logo/logo.png" class="textbook_logo" id="sidebar-logo" data-turbolinks-permanent/></a>
  <h2 class="c-sidebar__title">Geographic Data Science with Python</h2>
  <ul class="c-sidebar__chapters">
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/book/intro.html"
        >
          
          Home
        </a>

        
      </li>

      
    
      
      
        <li class="c-sidebar__chapter"><a class="c-sidebar__entry" href="/book/search.html">Search</a></li>
        
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/book/notebooks/00_toc.html"
        >
          
          Table of Contents
        </a>

        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/book/intro_part_i.html"
        >
          
          Part I - Building Blocks
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/01_geospatial_computational_environment.html"
                >
                  
                  Geospatial Computational Environment
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/02_spatial_data.html"
                >
                  
                  Spatial Data
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/03_spatial_data_processing.html"
                >
                  
                  Spatial Data Processing
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/04_spatial_weights.html"
                >
                  
                  Spatial Weights
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/book/intro_part_ii.html"
        >
          
          Part II - Spatial Data Analysis
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/05_choropleth.html"
                >
                  
                  Choropleth Mapping
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/06_spatial_autocorrelation.html"
                >
                  
                  Spatial Autocorrelation
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/07_local_autocorrelation.html"
                >
                  
                  Local Spatial Autocorrelation
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/08_point_pattern_analysis.html"
                >
                  
                  Point Pattern Analysis
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
      
      

      
      

      
      
      <li class="c-sidebar__chapter">
        <a class="c-sidebar__entry "
          href="/book/intro_part_iii.html"
        >
          
          Part III - Advanced Topics
        </a>

        

          
          
          
          

          

          <ul class="c-sidebar__sections ">
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/09_spatial_inequality.html"
                >
                  
                  Spatial Inequality
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry c-sidebar__entry--active"
                  href="/book/notebooks/10_clustering_and_regionalization.html"
                >
                  
                  Clustering & Regionalization
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/11_regression.html"
                >
                  
                  Regression
                </a>

                
                

              </li>
              
            
              
              

              <li class="c-sidebar__section">
                <a class="c-sidebar__entry "
                  href="/book/notebooks/12_feature_engineering.html"
                >
                  
                  Spatial Feature Engineering
                </a>

                
                

              </li>
              
            
          </ul>
        
      </li>

      
    
  </ul>
  <p class="sidebar_footer">Powered by <a href="https://github.com/jupyter/jupyter-book">Jupyter Book</a></p>
</nav>

      
      <!-- Empty sidebar placeholder that we'll auto-fill with javascript -->
      <aside class="sidebar__right">
          <header><h4 class="nav__title"><i class="fa fa-list"></i>   On this page</h4></header>
          <nav class="onthispage">
          </nav>
      </aside>
      
      <main class="c-textbook__page" tabindex="-1">
          <div class="o-wrapper">
            <div class="c-sidebar-toggle">
  <!-- We show the sidebar by default so we use .is-active -->
  <button
    id="js-sidebar-toggle"
    class="hamburger hamburger--arrowalt is-active"
  >
    <span class="hamburger-box">
      <span class="hamburger-inner"></span>
    </span>
    <span class="c-sidebar-toggle__label">Toggle Sidebar</span>
  </button>
</div>

            
<div class="buttons">

<button id="interact-button-thebelab" class="interact-button">Thebelab</button>









<a href="https://mybinder.org/v2/gh/gdsbook/geographic-data-science/master?urlpath=lab/tree/content%2Fnotebooks%2F10_clustering_and_regionalization.ipynb"><button class="interact-button" id="interact-button-binder"><img class="interact-button-logo" src="/book/assets/images/logo_binder.svg" alt="Interact" />Interact</button></a>


</div>


            <div class="c-textbook__content">
              <h1 id="clustering-and-regionalization">Clustering and Regionalization</h1>
<!--
**NOTE**: parts of this notebook have been
borrowed from [GDS'17 - Lab
6](http://darribas.org/gds17/content/labs/lab_06.html)
-->

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pysal.explore.esda.moran</span> <span class="kn">import</span> <span class="n">Moran</span>
<span class="kn">from</span> <span class="nn">pysal.lib.api</span> <span class="kn">import</span> <span class="n">Queen</span><span class="p">,</span> <span class="n">KNN</span>
<span class="kn">from</span> <span class="nn">pysal.lib.weights</span> <span class="kn">import</span> <span class="n">Wsets</span>
<span class="kn">from</span> <span class="nn">booktools</span> <span class="kn">import</span> <span class="n">choropleth</span>
<span class="kn">import</span> <span class="nn">seaborn</span> 
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">geopandas</span> 
<span class="kn">import</span> <span class="nn">data</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span><span class="p">,</span> <span class="n">AgglomerativeClustering</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

</code></pre></div>    </div>
  </div>

</div>

<h2 id="introduction">Introduction</h2>

<p>The world’s hardest questions are complex and multi-faceted.
Effective methods to learn from data should recognize this. Many questions
and challenges are inherently multidimensional; they are affected, shaped, and
defined by many different components all acting simultaneously. In statistical
terms, these processes are called <em>multivariate processes</em>, as opposed to 
<em>univariate processes</em>, where only a single variable acts at once.
Clustering is a fundamental method of geographical analysis that draws insights
from large, complex multivariate processes. It works by finding similarities among the
many dimensions in a multivariate process, condensing them down into a simpler representation
Thus, through clustering, a complex and difficult to understand process is recast into a simpler one
that even non-technical audiences can look at and understand.</p>

<p>Often, clustering involves sorting observations into groups. For these groups to be more
meaningful than any single initial dimension, members of a group should be more
similar to one another than they are to members of a different group.
Each group is referred to as a <em>cluster</em> while the process of assigning
objects to groups is known as <em>clustering</em>. If done well, these clusters can be
characterized by their <em>profile</em>, a simple summary of what members of a group
are like in terms of the original multivariate process.</p>

<p>Since a good cluster is more
similar internally than it is to any other cluster, these cluster-level profiles
provide a convenient shorthand to describe the original complex multivariate process.
Observations in one group may have consistently high 
scores on some traits but low scores on others. 
The analyst only needs to look at the profile of a cluster in order to get a
good sense of what all the observations in that cluster are like, instead of
having to consider all of the complexities of the original multivariate process at once. 
Throughout data science, and particularly in geographic data science, 
clustering is widely used to provide insights on the
geographic structure of complex multivariate spatial data.</p>

<p>In the context of explicitly spatial questions, a related concept, the <em>region</em>,
is also instrumental. A <em>region</em> is similar to a <em>cluster</em>, in the sense that
all members of a region have been grouped together, and the region should provide 
a shorthand for the original data. 
Further, for a region to be analytically useful, its members also should
display stronger similarity to each other than they do to the members of other regions. 
However, regions are more complex than clusters because they combine this
similarity in profile with additional information about the geography of their members.
In short, regions are like clusters (since they have a coherent profile), but they
also have a coherent geography—members of a region should also be
located near one another.</p>

<p>The process of creating regions is called regionalization.
A regionalization is a special kind of clustering where the objective is 
to group observations which are similar in their statistical attributes,
but also in their spatial location. In this sense, regionalization embeds the same
logic as standard clustering techniques, but also applies a series of
spatial and/or geographical constraints. Often, these
constraints relate to connectivity: two candidates can only be grouped together in the
same region if there exists a path from one member to another member
that never leaves the region. These paths often model the spatial relationships
in the data, such as contiguity or proximity. However, connectivity does not
always need to hold for all regions, and in certain contexts it makes
sense to relax connectivity or to impose different types of spatial constraints.</p>

<p>In this chapter we consider clustering techniques and regionalization methods which will
allow us to do exactly that. In the process, we will explore the characteristics
of neighborhoods in San Diego.
We will extract common patterns from the
cloud of multidimensional data that the Census Bureau produces about small areas
through the American Community Survey. We begin with an exploration of the
multivariate data about San Diego by suggesting some ways to examine the 
statistical and spatial distribution of the data before carrying out any
 clustering. Focusing on the individual variables, as well as their pairwise
associations, can help guide the subsequent application of clusterings or regionalizations.</p>

<p>We then consider geodemographic approaches to clustering—the application
of multivariate clustering to spatially referenced demographic data.
Two popular clustering algorithms are employed: k-means and Ward’s hierarchical method.
Mapping the spatial distribution of the resulting clusters 
reveals interesting insights on the socioeconomic structure of the San Diego
metropolitan area. We also see that in many cases, clusters are spatially 
fragmented. That is, a cluster may actually consist of different areas that are not
spatially connected. Indeed, some clusters will have their members strewn all over the map. 
This will illustrate why connectivity might be important when building insight
about spatial data, since these clusters will not at all provide intelligible regions. 
So, we then will move on to regionalization, exploring different approaches that
incorporate geographical constraints into the exploration of the social structure of San Diego.</p>

<h2 id="data">Data</h2>

<p>The dataset we will use in this chapter comes from the American Community Survey
(ACS). In particular, we examine data at the Census Tract level in San Diego,
California in 2016. Let us begin by reading in the data as a GeoDataFrame and
exploring the attribute names.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Read file</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">geopandas</span><span class="o">.</span><span class="n">read_file</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">san_diego_tracts</span><span class="p">())</span>
<span class="c"># Print column names</span>
<span class="n">db</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['GEOID', 'GEOID_x', 'Total Popu', 'White', 'Black', 'Hispanic',
       'Med Age', 'Pop &lt; 18', 'Total Hous', 'In househo', 'Male house',
       'Female hou', 'High Sch D', 'GED', 'Bachelor's', 'Travel tim',
       'Income bel', 'Income Mal', ' Income Fe', 'Median HH', 'Cash publi',
       'Lowest qui', 'Second qui', 'Third  qui', 'Fourth  qu', 'Lower limi',
       'Gini index', 'Total Empl', 'In Labor F', 'Civilian L', 'Employed',
       'Total Ho_1', 'Vacant', 'Owner Occu', 'Renter Occ', 'Median Num',
       'Median Str', 'Median Gro', 'Median Val', 'state', 'county', 'tract',
       'AREALAND', 'AREAWATER', 'BASENAME', 'CENTLAT', 'CENTLON', 'COUNTY_1',
       'FUNCSTAT', 'GEOID_y', 'INTPTLAT', 'INTPTLON', 'LSADC', 'MTFCC', 'NAME',
       'OBJECTID', 'OID', 'STATE_1', 'TRACT_1', 'renter_pct', 'geometry'],
      dtype='object')
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>While the ACS comes with a large number of attributes we can use for clustering
and regionalization, we are not limited to the original variables at hand; we
can construct additional variables. This is particularly useful when
we want to compare areas that are not very similar in some structural
characteristic, such as area or population. For example, a quick look into the
variable names shows most variables are counts. For tracts of different sizes,
these variables will mainly reflect their overall population, rather than provide direct information
about the variables itself. To get around this, we will cast many of these count variables to rates,
and use them in addition to a subset of the original variables. 
Together, this set of constructed and received variables will to
will be used for our clustering and regionalization.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Pull out total house units</span>
<span class="n">total_units</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="s">'Total Ho_1'</span><span class="p">]</span>
<span class="c"># Calculate percentage of renter occupied units</span>
<span class="n">pct_rental</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="s">'Renter Occ'</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_units</span> <span class="o">+</span> <span class="p">(</span><span class="n">total_units</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># Pull out total number of households</span>
<span class="n">total_hh</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="s">'Total Hous'</span><span class="p">]</span>
<span class="c"># Calculate percentage of female households</span>
<span class="n">pct_female_hh</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="s">'Female hou'</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">total_hh</span> <span class="o">+</span> <span class="p">(</span><span class="n">total_hh</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># Calculate percentage of population with a bachelor degree</span>
<span class="n">pct_bachelor</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="s">"Bachelor's"</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="s">'Total Popu'</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="s">'Total Popu'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span><span class="p">)</span>
<span class="c"># Assign newly created variables to main table `db`</span>
<span class="n">db</span><span class="p">[</span><span class="s">'pct_rental'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pct_rental</span>
<span class="n">db</span><span class="p">[</span><span class="s">'pct_female_hh'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pct_female_hh</span>
<span class="n">db</span><span class="p">[</span><span class="s">'pct_bachelor'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pct_bachelor</span>
<span class="c"># Calculate percentage of population white</span>
<span class="n">db</span><span class="p">[</span><span class="s">'pct_white'</span><span class="p">]</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="s">"White"</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="s">'Total Popu'</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="s">'Total Popu'</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>To make things easier later on, let us collect the variables we will use to
characterize Census tracts. These variables capture different aspects of the socio-
economic reality of each area and, taken together, they provide a comprehensive
characterization of San Diego as a whole:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cluster_variables</span> <span class="o">=</span>  <span class="p">[</span><span class="s">'Median Val'</span><span class="p">,</span>   <span class="c"># Median house value</span>
                      <span class="s">'pct_white'</span><span class="p">,</span>    <span class="c"># Percent of tract population that is white</span>
                      <span class="s">'pct_rental'</span><span class="p">,</span>   <span class="c"># Percent of households that are rented</span>
                      <span class="s">'pct_female_hh'</span><span class="p">,</span><span class="c"># Percent of female-led households </span>
                      <span class="s">'pct_bachelor'</span><span class="p">,</span> <span class="c"># Percent of tract population with a Bachelors degree</span>
                      <span class="s">'Median Num'</span><span class="p">,</span>   <span class="c"># Median number of rooms in the tract's households</span>
                      <span class="s">'Gini index'</span><span class="p">,</span>   <span class="c"># Gini index measuring tract wealth inequality</span>
                      <span class="s">'Med Age'</span><span class="p">,</span>      <span class="c"># Median age of tract population</span>
                      <span class="s">'Travel tim'</span>    <span class="c"># ???</span>
                      <span class="p">]</span>
</code></pre></div>    </div>
  </div>

</div>

<h3 id="exploring-the-data">Exploring the data</h3>

<p>Now let’s start building up our understanding of this
dataset through both visual and summary statistical measures.</p>

<p>We will start by
looking at the spatial distribution of each variable alone.
This will help us draw a picture of the multi-faceted view of the tracts we
want to capture with our clustering. Let’s use choropleth maps for the
nine attributes and compare these choropleth maps side-by-side:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="c"># Make the axes accessible with single indexing</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="c"># Start a loop over all the variables of interest</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cluster_variables</span><span class="p">):</span>
    <span class="c"># select the axis where the map will go</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="c"># Plot the map</span>
    <span class="c">#db.plot(column=col, ax=ax, scheme='Quantiles', </span>
    <span class="c">#        linewidth=0, cmap='RdPu')</span>
    <span class="n">choropleth</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'RdPu'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="c"># Remove axis clutter</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
    <span class="c"># Set the axis title to the name of variable being plotted</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>
<span class="c"># Display the figure</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_9_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Many visual patterns jump out from the maps, revealing both commonalities as
well as differences across the spatial distributions of the individual variables.
Several variables tend to increase in value from the east to the west
(<code class="highlighter-rouge">pct_rental</code>, <code class="highlighter-rouge">Median Val</code>, <code class="highlighter-rouge">Median Num</code>, and <code class="highlighter-rouge">Travel tim</code>) while others
have a spatial trend in the opposite direction (<code class="highlighter-rouge">pct_white</code>, <code class="highlighter-rouge">pct_female_hh</code>,
<code class="highlighter-rouge">pct_bachelor</code>, <code class="highlighter-rouge">Med Age</code>). This is actually desirable; when variables have
different spatial distributions, each variable to contributes distinct 
information to the profiles of each cluster. However, if all variables display very similar 
spatial patterns, the amount of useful information across the maps is 
actually smaller than it appears, so cluster profiles may be much less useful as well.
It is also important to consider whether the variables display any
spatial autocorrelation, as this will affect the spatial structure of the
resulting clusters.</p>

<p>Recall from chapter XXX that Moran’s I is a commonly used
measure for global spatial autocorrelation. 
Let us get a quick sense to what
extent this is present in our dataset.
First, we need to build a spatial weights matrix that encodes the spatial
relationships in our San Diego data. We will start with queen contiguity:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w_queen</span> <span class="o">=</span> <span class="n">Queen</span><span class="o">.</span><span class="n">from_dataframe</span><span class="p">(</span><span class="n">db</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>As the warning tells us, observation <code class="highlighter-rouge">103</code> is an <em>island</em>, a disconnected observation
with no queen contiguity neighbors. To make sure that every observation
has at least one neighbor, we can combine the queen contiguity matrix with a
nearest neighbor matrix. This would ensure that every observation is neighbor 
of at least the observation it is closest to, plus all the areas with which 
it shares any border. Let’s first create the <code class="highlighter-rouge">KNN-1 W</code>:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w_k1</span> <span class="o">=</span> <span class="n">KNN</span><span class="o">.</span><span class="n">from_dataframe</span><span class="p">(</span><span class="n">db</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now we can combine the queen and nearest neighbor matrices into a single representation
with no disconnected observations. This full-connected connectivity matrix is the 
one we will use for analysis:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">Wsets</span><span class="o">.</span><span class="n">w_union</span><span class="p">(</span><span class="n">w_queen</span><span class="p">,</span> <span class="n">w_k1</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>As we ensured (thanks to the nearest neighbor connections),  <code class="highlighter-rouge">w</code> does not contain
any islands:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span><span class="o">.</span><span class="n">islands</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[]
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Now let’s calculate Moran’s I for the variables being used. This will measure
the extent to which each variable contains spatial structure:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set seed for reproducibility</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123456</span><span class="p">)</span>
<span class="c"># Calculate Moran's I for each variable</span>
<span class="n">mi_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">Moran</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">variable</span><span class="p">],</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">cluster_variables</span><span class="p">]</span>
<span class="c"># Display on table</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">variable</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">I</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">p_sim</span><span class="p">)</span> <span class="k">for</span> <span class="n">variable</span><span class="p">,</span><span class="n">res</span> 
                      <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cluster_variables</span><span class="p">,</span> <span class="n">mi_results</span><span class="p">)],</span>
                     <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'Variable'</span><span class="p">,</span> <span class="s">"Moran's I"</span><span class="p">,</span> <span class="s">'P-value'</span><span class="p">])</span>\
          <span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'Variable'</span><span class="p">)</span>
<span class="n">table</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Moran's I</th>
      <th>P-value</th>
    </tr>
    <tr>
      <th>Variable</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Median Val</th>
      <td>0.664557</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>pct_white</th>
      <td>0.691054</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>pct_rental</th>
      <td>0.465938</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>pct_female_hh</th>
      <td>0.297972</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>pct_bachelor</th>
      <td>0.421799</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>Median Num</th>
      <td>0.554416</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>Gini index</th>
      <td>0.293869</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>Med Age</th>
      <td>0.399902</td>
      <td>0.001</td>
    </tr>
    <tr>
      <th>Travel tim</th>
      <td>0.096992</td>
      <td>0.001</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<p>Each of the variables displays significant positive spatial autocorrelation,
suggesting that Tobler’s law is alive and well in the socioeconomic geography of San
Diego County. This means we also should expect the clusters we find will have
a non random spatial distribution. In particular, we would expect clusters to have
a modest amount of spatial coherence in addition to the coherence in their profiles,
since there is strong positive autocorrelation in all of the input variables.</p>

<p>Spatial autocorrelation only describes relationships between a single observation at a time.
So, the fact that all of the clustering variables are positively autocorrelated does not tell us 
about the way the attributes covary over space. For that, we need to consider the
spatial correlation between variables. Here, we will measure this using the
bivariate correlation in the maps of covariates themselves.</p>

<p>Given the 9 maps, there are 36 pairs of maps that must be compared. This is too 
many maps to process visually, so we can turn to an alternative tool to
explicitly focus on the bivariate relations between each pair of attributes.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="s">'reg'</span><span class="p">,</span> <span class="n">diag_kind</span><span class="o">=</span><span class="s">'kde'</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_21_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Two different types of plots are contained in the scatterplot matrix. On the
diagonal are the density functions for the nine attributes. These allow for an
inspection of the overall morphology of the attribute’s value distribution.
Examining these we see that our selection of variables includes those that are
negatively skewed (<code class="highlighter-rouge">pct_white</code> and <code class="highlighter-rouge">pct_female_hh</code>) as well as positively skewed
(<code class="highlighter-rouge">while median_val</code>, <code class="highlighter-rouge">pct_bachelor</code>, and <code class="highlighter-rouge">travel_tim</code>).</p>

<p>The second type of visualization lies in the off-diagonal cells of the matrix; 
these are bi-variate scatterplots. Each cell shows the association between one
pair of variables. Several of these cells indicate positive linear
associations (<code class="highlighter-rouge">med_age</code> Vs. <code class="highlighter-rouge">median_value</code>, <code class="highlighter-rouge">median_value</code> Vs. <code class="highlighter-rouge">Median Num</code>)
while other cells display negative correlation (<code class="highlighter-rouge">Median Val</code> Vs. <code class="highlighter-rouge">pct_rental</code>,
<code class="highlighter-rouge">Median Num</code> Vs. <code class="highlighter-rouge">pct_rental</code>, and <code class="highlighter-rouge">Med Age</code> Vs. <code class="highlighter-rouge">pct_rental</code>). The one variable
that tends to have consistenty weak association with the other variables is
<code class="highlighter-rouge">Travel tim</code>, and in part this appears to reflect its rather concentrated 
distribution as seen on the lower right diagonal corner cell.</p>

<h2 id="geodemographic-clusters-in-san-diego-census-tracts">Geodemographic Clusters in San Diego Census Tracts</h2>

<p>We now will move
beyond the implicitly bi-variate focus to consider the full multidimensional
nature of this data set. Geodemographic analysis is a form of multivariate
clustering where the observations represent geographical areas. The output
of these clusterings is nearly always mapped. Altogether, these methods use
multivariate clustering algorithms to construct a known number of
clusters ($k$), where the number of clusters is typically much smaller than the 
number of observations to be clustered. Each cluster is given a unique label,
and these labels are mapped. Using the clusters’ profile and label, the map of 
labels can be interpreted to get a sense of the spatial distribution of 
sociodemographic traits. The power of (geodemographic) clustering comes
from taking statistical variation across several dimensions and compressing it
into a single categorical one that we can visualize through a map. To
demonstrate the variety of approaches in clustering, we will show two
distinct but very popular clustering algorithms: k-means and Ward’s hierarchical method.</p>

<h3 id="k-means">K-means</h3>

<p>K-means is probably the most widely used approach to
cluster a dataset. The algorithm groups observations into a
prespecified number of clusters so that that each observation is
closer to the mean of its own cluster than it is to the mean of any other cluster.
The k-means problem is solved by iterating between an assignment step and an update step. 
First, all observations are randomly assigned one of the $k$ labels. Next, the 
multivariate mean over all covariates is calculated for each of the clusters.
Then, each observation is reassigned to the cluster with the closest mean. 
If the observation is already assigned to the cluster whose mean it is closest to,
the observation remains in that cluster. This assignment-update process continues
until no further reassignments are necessary.</p>

<p>The nature of this algorithm requires us to select the number of clusters we 
want to create. The right number of clusters is unknown in practice. For
illustration, we will use $k=5$ in the <code class="highlighter-rouge">KMeans</code> implementation from
<code class="highlighter-rouge">scikit-learn</code>. To proceed, we first create a <code class="highlighter-rouge">KMeans</code> clusterer:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Initialise KMeans instance</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Next, we call the <code class="highlighter-rouge">fit</code> method to actually apply the k-means algorithm to our data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set the seed for reproducibility</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>
<span class="c"># Run K-Means algorithm</span>
<span class="n">k5cls</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">])</span>
</code></pre></div>    </div>
  </div>

</div>

<p>Now that the clusters have been assigned, we can examine the label vector, which 
records the cluster to which each observation is assigned:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">k5cls</span><span class="o">.</span><span class="n">labels_</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([3, 4, 4, 0, 0, 4, 0, 0, 0, 2, 0, 2, 2, 2, 4, 0, 2, 2, 2, 4, 4, 4,
       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 0,
       0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,
       4, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,
       2, 2, 4, 4, 4, 0, 0, 0, 0, 2, 2, 0, 2, 4, 4, 0, 2, 0, 0, 0, 4, 0,
       0, 0, 0, 0, 2, 0, 4, 4, 3, 3, 4, 4, 0, 0, 4, 4, 4, 4, 4, 2, 4, 0,
       4, 4, 4, 4, 4, 3, 1, 3, 4, 1, 0, 4, 4, 3, 1, 1, 3, 3, 4, 3, 0, 4,
       4, 3, 4, 4, 4, 0, 4, 0, 0, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,
       2, 0, 2, 2, 2, 0, 2, 4, 4, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 0,
       2, 2, 0, 0, 2, 2, 0, 4, 0, 4, 4, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 3,
       4, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2,
       3, 3, 1, 3, 3, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0,
       0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0,
       0, 4, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0,
       0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 2, 0, 4, 0, 0, 4, 2, 0, 2, 0, 2,
       2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0,
       2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 0, 0, 4, 0, 0, 0,
       0, 2, 4, 4, 4, 0, 3, 4, 0, 4, 4, 0, 4, 0, 4, 0, 0, 0, 4, 4, 4, 4,
       4, 4, 0, 0, 0, 0, 4, 4, 4, 4, 0, 4, 1, 4, 4, 4, 3, 1, 3, 4, 3, 3,
       3, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 0, 0, 4, 0,
       4, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 2,
       0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 4, 0, 2,
       0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0,
       4, 4, 0, 4, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 4, 2, 2, 0, 2, 2, 0, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 0, 4, 2, 0, 4, 2, 2,
       2, 0, 4, 2, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 4, 0,
       0, 0, 4, 4, 4, 3, 3, 1, 2, 2, 4], dtype=int32)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>In this case, the second and third observations are assigned to cluster 4, while
the fourth and fifth observations have been placed in cluster 0. It is important
to note that the integer labels should be viewed as denoting membership only —
the numerical differences between the values for the labels are meaningless.
The profiles of the various clusters must be further explored by looking
at the values of each dimension.</p>

<p>But, before we do that, let’s make a map.</p>

<h3 id="spatial-distribution-of-clusters">Spatial Distribution of Clusters</h3>

<p>Having obtained the cluster labels, we can display the spatial
distribution of the clusters by using the labels as the categories in a
choropleth map. This allows us to quickly grasp any sort of spatial pattern the 
clusters might have. Since clusters represent areas with similar
characteristics, mapping their labels allows to see to what extent similar areas tend
to have similar locations.
Thus, this gives us one map that incorporates the information of from all nine covariates.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Assign labels into a column</span>
<span class="n">db</span><span class="p">[</span><span class="s">'k5cls'</span><span class="p">]</span> <span class="o">=</span> <span class="n">k5cls</span><span class="o">.</span><span class="n">labels_</span>
<span class="c"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="c"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s">'k5cls'</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c"># Keep axes proportionate</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
<span class="c"># Add title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">r'Geodemographic Clusters (k-means, $k=5$)'</span><span class="p">)</span>
<span class="c"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_29_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>The map provides a useful view of the clustering results; it allows for
a visual inspection of the extent to which Tobler’s first law of geography is
reflected in the multivariate clusters. Recall that the law implies that nearby
tracts should be more similar to one another than tracts that are geographically
more distant from each other. We can see evidence of this in
our cluster map, since clumps of tracts with the same color emerge. However, this
visual inspection is obscured by the complexity of the underlying spatial
units. Our eyes are drawn to the larger polygons in the eastern part of the
county, giving the impression that cluster 2 is the dominant cluster. While this
seems to be true in terms of land area (and we will verify this below), there is
more to the cluster pattern than this. Because the tract polygons are all 
different sizes and shapes, we cannot solely rely on our eyes to interpret 
the spatial distribution of clusters.</p>

<h3 id="statistical-analysis-of-the-cluster-map">Statistical Analysis of the Cluster Map</h3>

<p>To complement the geovisualization of the clusters, we can explore the
statistical properties of the cluster map. This process allows us to delve
into what observations are part of each cluster and what their
characteristics are.
This gives us the profile of each cluster so we can interpret the meaning of the
labels we’ve obtained. We can start, for example, by
considering cardinality, or the count of observations in each cluster:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Group data table by cluster label and count observations</span>
<span class="n">k5sizes</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'k5cls'</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">k5sizes</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k5cls
0    232
1      8
2    247
3     24
4    116
dtype: int64
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>And we can get a visual representation of cardinality as well:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span> <span class="o">=</span> <span class="n">k5sizes</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_33_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>There are substantial differences in the sizes of the five clusters, with two very
large clusters (0, 2), one medium sized cluster (4), and two small clusters (1,
3). Cluster 2 is the largest when measured by the number of assigned tracts.
This confirms our intuition from the map above, where we got the visual impression
that tracts in cluster 2 seemed to have the largest area. Let’s see if this is 
the case. To do so we can use the <code class="highlighter-rouge">dissolve</code> operation in <code class="highlighter-rouge">geopandas</code>, which 
combines all tracts belonging to each cluster into a single
polygon object. After we have dissolved all the members of the clusters,
we report the total land area of the cluster:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Dissolve areas by Cluster, aggregate by summing, and keep column for area</span>
<span class="n">areas</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">dissolve</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s">'k5cls'</span><span class="p">,</span> <span class="n">aggfunc</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)[</span><span class="s">'AREALAND'</span><span class="p">]</span>
<span class="n">areas</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>k5cls
0    3445601816
1      53860347
2    5816736150
3     220120882
4     752511344
Name: AREALAND, dtype: int64
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>And, to show this visually:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">areas</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1a28a8c3c8&gt;
</code></pre></div>      </div>

    </div>
  </div>
  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_37_1.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Our visual impression is confirmed: cluster 2 contains tracts that
together comprise 5,816,736,150 square meters (approximately 2,245 square miles),
which accounts for over half of the total land area in the county:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">areas</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="n">areas</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.5653447326157774
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Let’s move on to build the profiles for each cluster. Again, the profiles is what
provides the conceptual shorthand, moving from the arbitrary label to a meaningful
collection of observations with similar attributes. To build a basic profile, we can
compute the means of each of the attributes in every cluster:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Group table by cluster label, keep the variables used </span>
<span class="c"># for clustering, and obtain their mean</span>
<span class="n">k5means</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'k5cls'</span><span class="p">)[</span><span class="n">cluster_variables</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">k5means</span><span class="o">.</span><span class="n">T</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>k5cls</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Median Val</th>
      <td>430287.707171</td>
      <td>1.700850e+06</td>
      <td>272717.408907</td>
      <td>1.073158e+06</td>
      <td>658511.206897</td>
    </tr>
    <tr>
      <th>pct_white</th>
      <td>0.725640</td>
      <td>9.253820e-01</td>
      <td>0.651341</td>
      <td>8.485452e-01</td>
      <td>0.803962</td>
    </tr>
    <tr>
      <th>pct_rental</th>
      <td>0.405961</td>
      <td>2.344535e-01</td>
      <td>0.520519</td>
      <td>2.839565e-01</td>
      <td>0.332000</td>
    </tr>
    <tr>
      <th>pct_female_hh</th>
      <td>0.097588</td>
      <td>1.180614e-01</td>
      <td>0.105221</td>
      <td>1.012079e-01</td>
      <td>0.098318</td>
    </tr>
    <tr>
      <th>pct_bachelor</th>
      <td>0.009961</td>
      <td>1.874663e-03</td>
      <td>0.019729</td>
      <td>2.455057e-03</td>
      <td>0.004084</td>
    </tr>
    <tr>
      <th>Median Num</th>
      <td>5.349009</td>
      <td>6.437500e+00</td>
      <td>4.692308</td>
      <td>6.079167e+00</td>
      <td>5.688793</td>
    </tr>
    <tr>
      <th>Gini index</th>
      <td>0.400495</td>
      <td>5.257750e-01</td>
      <td>0.402543</td>
      <td>4.676458e-01</td>
      <td>0.423886</td>
    </tr>
    <tr>
      <th>Med Age</th>
      <td>37.031940</td>
      <td>5.057500e+01</td>
      <td>33.447368</td>
      <td>4.497083e+01</td>
      <td>41.429310</td>
    </tr>
    <tr>
      <th>Travel tim</th>
      <td>2438.086207</td>
      <td>1.215125e+03</td>
      <td>2072.421053</td>
      <td>2.056792e+03</td>
      <td>2289.508621</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<p>We see that cluster 3, for example, is composed of tracts that have
the highest average <code class="highlighter-rouge">Median_val</code>, while cluster 2 has the highest level of inequality
(<code class="highlighter-rouge">Gini index</code>), and cluster 1 contains an older population (<code class="highlighter-rouge">Med Age</code>)
who tend to live in housing units with more rooms (<code class="highlighter-rouge">Median Num</code>).
Average values, however, can hide a great deal of detail and, in some cases,
give wrong impressions about the type of data distribution they represent. To
obtain more detailed profiles, we can use the <code class="highlighter-rouge">describe</code> command in <code class="highlighter-rouge">pandas</code>, 
after grouping our observations by their clusters:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Group table by cluster label, keep the variables used </span>
<span class="c"># for clustering, and obtain their descriptive summary</span>
<span class="n">k5desc</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'k5cls'</span><span class="p">)[</span><span class="n">cluster_variables</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
<span class="c"># Loop over each cluster and print a table with descriptives</span>
<span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">k5desc</span><span class="o">.</span><span class="n">T</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\t</span><span class="s">---------</span><span class="se">\n\t</span><span class="s">Cluster </span><span class="si">%</span><span class="s">i'</span><span class="o">%</span><span class="n">cluster</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">k5desc</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="n">cluster</span><span class="p">]</span><span class="o">.</span><span class="n">unstack</span><span class="p">())</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">
      <div class="output_stream highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
	---------
	Cluster 0
               count           mean           std          min            25%  \
Median Val     232.0  430287.707171  47636.060405  354400.0000  385050.000000   
pct_white      232.0       0.725640      0.143014       0.0000       0.665323   
pct_rental     232.0       0.405961      0.219609       0.0000       0.231308   
pct_female_hh  232.0       0.097588      0.029461       0.0000       0.086273   
pct_bachelor   232.0       0.009961      0.010269       0.0000       0.002747   
Median Num     232.0       5.349009      0.976408       2.7000       4.800000   
Gini index     232.0       0.400495      0.060185       0.0128       0.365100   
Med Age        232.0      37.031940      7.360078      14.9000      33.400000   
Travel tim     232.0    2438.086207   1794.321201       0.0000    1617.000000   

                         50%            75%            max  
Median Val     427600.000000  463825.000000  543900.000000  
pct_white           0.745380       0.815653       0.965002  
pct_rental          0.381552       0.551725       1.000000  
pct_female_hh       0.102241       0.116436       0.166731  
pct_bachelor        0.007200       0.013292       0.058414  
Median Num          5.400000       6.100000       7.800000  
Gini index          0.404450       0.437100       0.574000  
Med Age            36.500000      40.800000      71.500000  
Travel tim       2213.500000    2954.750000   21780.000000  

	---------
	Cluster 1
               count          mean            std           min           25%  \
Median Val       8.0  1.700850e+06  202293.816833  1.424300e+06  1.599575e+06   
pct_white        8.0  9.253820e-01       0.039039  8.565188e-01  9.067396e-01   
pct_rental       8.0  2.344535e-01       0.090575  1.058824e-01  1.461760e-01   
pct_female_hh    8.0  1.180614e-01       0.024708  8.609795e-02  9.646732e-02   
pct_bachelor     8.0  1.874663e-03       0.003391  0.000000e+00  0.000000e+00   
Median Num       8.0  6.437500e+00       0.919530  4.800000e+00  5.975000e+00   
Gini index       8.0  5.257750e-01       0.036813  4.547000e-01  5.069750e-01   
Med Age          8.0  5.057500e+01       3.143133  4.540000e+01  4.952500e+01   
Travel tim       8.0  1.215125e+03     375.770993  7.300000e+02  9.027500e+02   

                        50%           75%           max  
Median Val     1.704000e+06  1.777100e+06  2.000001e+06  
pct_white      9.347479e-01  9.496601e-01  9.718606e-01  
pct_rental     2.606813e-01  3.052264e-01  3.425729e-01  
pct_female_hh  1.201224e-01  1.363923e-01  1.556420e-01  
pct_bachelor   0.000000e+00  1.965965e-03  9.239815e-03  
Median Num     6.500000e+00  6.900000e+00  7.800000e+00  
Gini index     5.354000e-01  5.466250e-01  5.743000e-01  
Med Age        5.035000e+01  5.117500e+01  5.670000e+01  
Travel tim     1.281000e+03  1.398000e+03  1.854000e+03  

	---------
	Cluster 2
               count           mean           std           min  \
Median Val     247.0  272717.408907  68847.844348  16600.000000   
pct_white      247.0       0.651341      0.190667      0.101900   
pct_rental     247.0       0.520519      0.211114      0.109715   
pct_female_hh  247.0       0.105221      0.020948      0.038137   
pct_bachelor   247.0       0.019729      0.013652      0.000000   
Median Num     247.0       4.692308      0.852723      2.700000   
Gini index     247.0       0.402543      0.046071      0.262600   
Med Age        247.0      33.447368      5.470630     24.300000   
Travel tim     247.0    2072.421053    677.155331    550.000000   

                         25%            50%            75%            max  
Median Val     245050.000000  293000.000000  319800.000000  351200.000000  
pct_white           0.583502       0.699765       0.788069       0.927177  
pct_rental          0.336651       0.539113       0.685844       0.927035  
pct_female_hh       0.092346       0.103783       0.119947       0.170026  
pct_bachelor        0.010110       0.017334       0.028896       0.068300  
Median Num          4.000000       4.600000       5.400000       6.500000  
Gini index          0.373000       0.400300       0.429800       0.585300  
Med Age            29.400000      32.500000      36.450000      65.400000  
Travel tim       1609.500000    2003.000000    2460.500000    4659.000000  

	---------
	Cluster 3
               count          mean            std            min  \
Median Val      24.0  1.073158e+06  120340.198628  881700.000000   
pct_white       24.0  8.485452e-01       0.105947       0.586509   
pct_rental      24.0  2.839565e-01       0.161366       0.079941   
pct_female_hh   24.0  1.012079e-01       0.018171       0.063891   
pct_bachelor    24.0  2.455057e-03       0.002963       0.000000   
Median Num      24.0  6.079167e+00       1.375557       3.900000   
Gini index      24.0  4.676458e-01       0.042361       0.385800   
Med Age         24.0  4.497083e+01       8.540974      27.100000   
Travel tim      24.0  2.056792e+03    1237.905278     804.000000   

                         25%           50%           75%           max  
Median Val     964325.000000  1.098200e+06  1.131350e+06  1.354700e+06  
pct_white           0.842347  8.878381e-01  9.179736e-01  9.385125e-01  
pct_rental          0.145002  2.216039e-01  4.490072e-01  5.808486e-01  
pct_female_hh       0.092296  1.021899e-01  1.144775e-01  1.359801e-01  
pct_bachelor        0.000000  9.721194e-04  4.536051e-03  1.022677e-02  
Median Num          5.000000  6.400000e+00  7.250000e+00  8.100000e+00  
Gini index          0.445450  4.670000e-01  4.835250e-01  5.766000e-01  
Med Age            38.825000  4.555000e+01  5.355000e+01  5.630000e+01  
Travel tim       1152.750000  1.645500e+03  2.939250e+03  5.887000e+03  

	---------
	Cluster 4
               count           mean           std            min  \
Median Val     116.0  658511.206897  82853.851183  545600.000000   
pct_white      116.0       0.803962      0.122426       0.426606   
pct_rental     116.0       0.332000      0.199404       0.038530   
pct_female_hh  116.0       0.098318      0.024188       0.020283   
pct_bachelor   116.0       0.004084      0.005218       0.000000   
Median Num     116.0       5.688793      1.264001       2.300000   
Gini index     116.0       0.423886      0.056062       0.295500   
Med Age        116.0      41.429310      6.776468      21.700000   
Travel tim     116.0    2289.508621   1190.678772     692.000000   

                         25%            50%            75%            max  
Median Val     601750.000000  638900.000000  707225.000000  863700.000000  
pct_white           0.739922       0.844445       0.889507       0.960493  
pct_rental          0.166894       0.290281       0.453746       0.781067  
pct_female_hh       0.085454       0.097574       0.114602       0.166557  
pct_bachelor        0.000000       0.002867       0.005968       0.031444  
Median Num          4.575000       5.900000       6.800000       8.300000  
Gini index          0.385800       0.424550       0.465100       0.586100  
Med Age            36.850000      41.000000      47.125000      59.300000  
Travel tim       1484.500000    2077.500000    2794.250000    8339.000000  
</code></pre></div>      </div>
    </div>
  </div>
</div>

<p>However, this approach quickly gets out of hand: more detailed profiles can simply
return to an unwieldy mess of numbers. A better approach to constructing
cluster profiles is be to draw the distributions of cluster members’ data.
To do this we need to “tidy up” the dataset. A tidy dataset (<a href="https://www.jstatsoft.org/article/view/v059i10">Wickham,
2014</a>) is one where every row is
an observation, and every column is a variable. Thus, a few steps are required 
to tidy up our labelled data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Index db on cluster ID</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'k5cls'</span><span class="p">)</span>
<span class="c"># Keep only variables used for clustering</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">]</span>
<span class="c"># Stack column names into a column, obtaining </span>
<span class="c"># a "long" version of the dataset</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="c"># Take indices into proper columns</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c"># Rename column names</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span>
                        <span class="s">'level_1'</span><span class="p">:</span> <span class="s">'Attribute'</span><span class="p">,</span> 
                        <span class="mi">0</span><span class="p">:</span> <span class="s">'Values'</span><span class="p">})</span>
<span class="c"># Check out result</span>
<span class="n">tidy_db</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>k5cls</th>
      <th>Attribute</th>
      <th>Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>Median Val</td>
      <td>1.038200e+06</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>pct_white</td>
      <td>9.385125e-01</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>pct_rental</td>
      <td>7.994078e-02</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>pct_female_hh</td>
      <td>8.726068e-02</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>pct_bachelor</td>
      <td>0.000000e+00</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<p>Now we are ready to plot. Below, we’ll show the distribution of each cluster’s values
for each variable. This gives us the full distributional profile of each cluster:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Setup the facets</span>
<span class="n">facets</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tidy_db</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">'Attribute'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'k5cls'</span><span class="p">,</span> \
                  <span class="n">sharey</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c"># Build the plot from `sns.kdeplot`</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">facets</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="n">seaborn</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="s">'Values'</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_47_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>This allows us to see that, while some attributes such as the percentage of
female households (<code class="highlighter-rouge">pct_female_hh</code>) display largely the same distribution for
each cluster, others paint a much more divided picture (e.g. <code class="highlighter-rouge">Median Val</code>).
Taken altogether, these graphs allow us to start delving into the multidimensional 
complexity of each cluster and the types of areas behind them.</p>

<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>

<p>As mentioned above, k-means is only one clustering algorithm. There are
plenty more. In this section, we will take a similar look at the San Diego
dataset using another staple of the clustering toolkit: agglomerative
hierarchical clustering (AHC). Agglomerative clustering works by building a hierarchy of
clustering solutions that starts with all singletons (each observation is a single
cluster in itself) and ends with all observations assigned to the same cluster.
These extremes are not very useful in themselves. But, in between, the hierarchy
contains many distinct clustering solutions with varying levels of detail. 
The intuition behind the algorithm is also rather straightforward:</p>

<p>1) begin with everyone as part of its own cluster; 
2) find the two closest observations based on a distance metric (e.g. euclidean); 
3) join them into a new cluster; 
4) repeat steps 2) and 3) until reaching the degree of aggregation desired.</p>

<p>The algorithm is thus called “agglomerative”
because it starts with individual clusters and “agglomerates” them into fewer
and fewer clusters containing more and more observations each. Also, like with 
k-means, AHC does require the user to specify a number of clusters in advance.
This is because, following from the mechanism the method has to build clusters, 
AHC can provide a solution with as many clusters as observations ($k=n$),
or with a only one ($k=1$).</p>

<p>Enough of theory, let’s get coding! In Python, AHC can be run
with <code class="highlighter-rouge">scikit-learn</code> in very much the same way we did for k-means in the previous
section. In this case, we use the <code class="highlighter-rouge">AgglomerativeClustering</code> class and again 
use the <code class="highlighter-rouge">fit</code> method to actually apply the clustering algorithm to our data:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set seed for reproducibility</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c"># Iniciate the algorithm</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">linkage</span><span class="o">=</span><span class="s">'ward'</span><span class="p">,</span> <span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c"># Run clustering</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">])</span>
<span class="c"># Assign labels to main data table</span>
<span class="n">db</span><span class="p">[</span><span class="s">'ward5'</span><span class="p">]</span> <span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
</code></pre></div>    </div>
  </div>

</div>

<p>As above, we can check the number of observations that fall within each cluster:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ward5sizes</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'ward5'</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">ward5sizes</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ward5
0    134
1    326
2     16
3    145
4      6
dtype: int64
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Further, we can check the simple average profiles of our clusters:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ward5means</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'ward5'</span><span class="p">)[</span><span class="n">cluster_variables</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">ward5means</span><span class="o">.</span><span class="n">T</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>ward5</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Median Val</th>
      <td>673423.880597</td>
      <td>298094.171779</td>
      <td>1.193119e+06</td>
      <td>453176.883198</td>
      <td>1.786634e+06</td>
    </tr>
    <tr>
      <th>pct_white</th>
      <td>0.804847</td>
      <td>0.673469</td>
      <td>8.664667e-01</td>
      <td>0.713027</td>
      <td>9.341647e-01</td>
    </tr>
    <tr>
      <th>pct_rental</th>
      <td>0.319893</td>
      <td>0.492282</td>
      <td>2.994236e-01</td>
      <td>0.417104</td>
      <td>2.222373e-01</td>
    </tr>
    <tr>
      <th>pct_female_hh</th>
      <td>0.099066</td>
      <td>0.104657</td>
      <td>1.077750e-01</td>
      <td>0.093584</td>
      <td>1.201785e-01</td>
    </tr>
    <tr>
      <th>pct_bachelor</th>
      <td>0.004003</td>
      <td>0.017606</td>
      <td>1.985041e-03</td>
      <td>0.009723</td>
      <td>2.324019e-03</td>
    </tr>
    <tr>
      <th>Median Num</th>
      <td>5.763433</td>
      <td>4.832515</td>
      <td>5.887500e+00</td>
      <td>5.350138</td>
      <td>6.600000e+00</td>
    </tr>
    <tr>
      <th>Gini index</th>
      <td>0.425901</td>
      <td>0.401975</td>
      <td>4.849938e-01</td>
      <td>0.399595</td>
      <td>5.212333e-01</td>
    </tr>
    <tr>
      <th>Med Age</th>
      <td>41.945522</td>
      <td>34.262883</td>
      <td>4.521875e+01</td>
      <td>36.689725</td>
      <td>5.145000e+01</td>
    </tr>
    <tr>
      <th>Travel tim</th>
      <td>2273.664179</td>
      <td>2124.248466</td>
      <td>1.877125e+03</td>
      <td>2538.565517</td>
      <td>1.148167e+03</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<p>And again, we can create a plot of the profiles’ distributions (after properly 
tidying up):</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Index db on cluster ID</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'ward5'</span><span class="p">)</span>
<span class="c"># Keep only variables used for clustering</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">]</span>
<span class="c"># Stack column names into a column, obtaining </span>
<span class="c"># a "long" version of the dataset</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span>
<span class="c"># Take indices into proper columns</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c"># Rename column names</span>
<span class="n">tidy_db</span> <span class="o">=</span> <span class="n">tidy_db</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span>
                        <span class="s">'level_1'</span><span class="p">:</span> <span class="s">'Attribute'</span><span class="p">,</span> 
                        <span class="mi">0</span><span class="p">:</span> <span class="s">'Values'</span><span class="p">})</span>
<span class="c"># Check out result</span>
<span class="n">tidy_db</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output output_html">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ward5</th>
      <th>Attribute</th>
      <th>Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Median Val</td>
      <td>1.038200e+06</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>pct_white</td>
      <td>9.385125e-01</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>pct_rental</td>
      <td>7.994078e-02</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>pct_female_hh</td>
      <td>8.726068e-02</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>pct_bachelor</td>
      <td>0.000000e+00</td>
    </tr>
  </tbody>
</table>
</div>
</div>

    </div>
  </div>
</div>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Setup the facets</span>
<span class="n">facets</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tidy_db</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">'Attribute'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'ward5'</span><span class="p">,</span> \
                  <span class="n">sharey</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c"># Build the plot as a `sns.kdeplot`</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">facets</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="n">seaborn</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="s">'Values'</span><span class="p">,</span> <span class="n">shade</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">add_legend</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_56_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>For the sake of brevity, we will not spend much time on the plots above.
However, the interpretation is analogous to that of the k-means example.</p>

<p>On the spatial side, we can explore the geographical dimension of the
clustering solution by making a map the clusters:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">db</span><span class="p">[</span><span class="s">'ward5'</span><span class="p">]</span> <span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
<span class="c"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="c"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s">'ward5'</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c"># Keep axes proportionate</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
<span class="c"># Add title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Geodemographic Clusters (AHC, $k=5$)'</span><span class="p">)</span>
<span class="c"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_58_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>And, to make comparisons simpler, we can display both the k-means and the AHC
results side by side:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">db</span><span class="p">[</span><span class="s">'ward5'</span><span class="p">]</span> <span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
<span class="c"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s">'ward5'</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Set2'</span><span class="p">,</span> 
        <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c"># Keep axes proportionate</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
<span class="c"># Add title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'K-Means solution ($k=5$)'</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="c"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s">'k5cls'</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'Set3'</span><span class="p">,</span>
        <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c"># Keep axes proportionate</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
<span class="c"># Add title</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'AHC solution ($k=5$)'</span><span class="p">)</span>

<span class="c"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_60_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>While we must remember our earlier caveat about how irregular polygons can 
baffle our visual intuition, a closer visual inspection of the cluster geography
suggests a clear pattern: although they are not identical, both clusterings capture
very similar overall spatial structure. Furthermore, both solutions slightly violate 
Tobler’s law, since all of the clusters have disconnected components. The five
multivariate clusters in each case are actually composed of many disparate 
geographical areas, strewn around the map according only to the structure of the
data and not its geography. That is, in order to travel to
every tract belonging to a cluster, we would have to journey through
other clusters as well.</p>

<h2 id="spatially-constrained-hierarchical-clustering">Spatially Constrained Hierarchical Clustering</h2>

<p>Fragmented clusters are not intrinsically invalid, particularly if we are
interested in exploring the overall structure and geography of multivariate
data. However, in some cases, the application we are interested in might
require that all the observations in a class be spatially connected. For
example, when detecting communities or neighborhoods (as is sometimes needed when
drawing electoral or census boundaries), they are nearly always distinct 
self-connected areas, unlike our clusters shown above. To ensure that clusters are
not spatially fragmented, we turn to regionalization.</p>

<p>Regionalization methods are clustering techniques that impose a spatial constraints
on clusters. In other words, the result of a regionalization algorithm contains clusters with
areas that are geographically coherent, in addition to having coherent data profiles. 
Effectively, this means that regionalization methods construct clusters that are 
all internally-connected; these are the <em>regions</em>. Thus, a regions’ members must
be geographically <em>nested</em> within the region’s boundaries.</p>

<p>This type of nesting relationship is easy to identify
in the real world. For example, counties nest within states, or, in the UK, 
local super output areas (LSOAs) nest within middle super output areas (MSOAs). 
The difference between these real-world nestings and the output of a regionalization
algorithm is that the real-world nestings are aggregated according to administrative principles, but regions’ members are aggregated according to a statistical technique. In the same manner as the
clustering techniques explored above, these regionalization methods aggregate 
observations that are similar in their covariates; the profiles of regions are useful
in a similar manner as the profiles of clusters. But, in regionalization, the 
clustering is also spatially constrained, so the region profiles and members will
likely be different from the unconstrained solutions.</p>

<p>As in the non-spatial case, there are many different regionalization methods.
Each has a different way to measure (dis)similarity, how the similarity is used
to assign labels, how these labels are iteratively adjusted, and so on. However,
as with clustering algorithms, regionalization methods all share a few common traits.
In particular, they all take a set of input attributes and a representation of 
spatial connectivity in the form of a binary spatial weights matrix. Depending 
on the algorithm, they also require the desired number of output regions. For
illustration, we will take the AHC algorithm we have just used above, and apply 
an additional spatial constraint. In <code class="highlighter-rouge">scikit-learn</code>, this is done using
our spatial weights matrix as a <code class="highlighter-rouge">connectivity</code> option.
This will force the agglomerative algorithm to only allow observations to be grouped
in a cluster if they are also spatially connected:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123456</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">linkage</span><span class="o">=</span><span class="s">'ward'</span><span class="p">,</span>
                                            <span class="n">connectivity</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
                                            <span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">])</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',
            connectivity=&lt;627x627 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
	with 3961 stored elements in Compressed Sparse Row format&gt;,
            linkage='ward', memory=None, n_clusters=5,
            pooling_func=&lt;function mean at 0x10ab8af28&gt;)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>Let’s inspect the output:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">db</span><span class="p">[</span><span class="s">'ward5wq'</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
<span class="c"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="c"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s">'ward5wq'</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c"># Keep axes proportionate</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
<span class="c"># Add title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">r'Geodemographic Regions (Ward, $k=5$, Queen Contiguity)'</span><span class="p">)</span>
<span class="c"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_64_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Introducing the spatial constraint results in fully-connected clusters with much
more concentrated spatial distributions. From an initial visual impression, it might
appear that our spatial constraint has been violated: there are tracts for both cluster 0 and
cluster 1 that appear to be disconnected from the rest of their clusters.
However, closer inspection reveals that each of these tracts is indeed connected
to another tract in its own cluster by very narrow shared boundaries.</p>

<h3 id="changing-the-spatial-constraint">Changing the spatial constraint</h3>

<p>The spatial constraint in regionalization algorithms is structured by the
spatial weights matrix we use. An interesting
question is thus how the choice of weights influences the final region structure.
Fortunately, we can directly explore the impact that a change in the spatial weights matrix has on
regionalization. To do so, we use the same attribute data
but replace the Queen contiguity matrix with a spatial k-nearest neighbor matrix,
where each observation is connected to its four nearest observations, instead
of those it touches.</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">KNN</span><span class="o">.</span><span class="n">from_shapefile</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">san_diego_tracts</span><span class="p">(),</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>    </div>
  </div>

</div>

<p>With this matrix connecting each tract to the four closest tracts, we can run 
another AHC regionalization:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123456</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">linkage</span><span class="o">=</span><span class="s">'ward'</span><span class="p">,</span>
                                            <span class="n">connectivity</span><span class="o">=</span><span class="n">w</span><span class="o">.</span><span class="n">sparse</span><span class="p">,</span>
                                            <span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">db</span><span class="p">[</span><span class="n">cluster_variables</span><span class="p">])</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <div class="output_data_text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',
            connectivity=&lt;627x627 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
	with 2508 stored elements in Compressed Sparse Row format&gt;,
            linkage='ward', memory=None, n_clusters=5,
            pooling_func=&lt;function mean at 0x10ab8af28&gt;)
</code></pre></div>      </div>

    </div>
  </div>
</div>

<p>And plot the final regions:</p>

<div class="cell code_cell">
  <div class="input_area">
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">db</span><span class="p">[</span><span class="s">'ward5wknn'</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">labels_</span>
<span class="c"># Setup figure and ax</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="c"># Plot unique values choropleth including a legend and with no boundary lines</span>
<span class="n">db</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="s">'ward5wknn'</span><span class="p">,</span> <span class="n">categorical</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="c"># Remove axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
<span class="c"># Keep axes proportionate</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'equal'</span><span class="p">)</span>
<span class="c"># Add title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Geodemographic Regions (Ward, $k=5$, four nearest neighbors)'</span><span class="p">)</span>
<span class="c"># Display the map</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>    </div>
  </div>

  <div class="output_wrapper">
    <div class="output_subarea">

      <p class="output_png"><img src="../images/notebooks/10_clustering_and_regionalization_70_0.png" alt="png" /></p>

    </div>
  </div>
</div>

<p>Even though we have specified a spatial constraint, the constraint applies to the
connectivity graph modeled by our weights matrix. Therefore, using k-nearest neighbors
to constrain the agglomerative clustering may not result in regions that are connected
according to a different connectivity rule, such as the queen contiguity rule used
in the previous section. However, the regionalization here is fortuitous; even though
we used the 4-nearest tracts to constrain connectivity, all but one of the clusters, 
cluster 4, is <em>also</em> connected according to our earlier queen contiguity rule.</p>

<p>At first glance, this may seem counter-intuitive. We did specify the spatial
constraint, so our initial reaction is that the connectivity constraint is
violated. However, this is not the case, since the constraint applies to the
k-nearest neighbor graph, not the queen contiguity graph. Therefore, since tracts
in this solution are considered as connected to their four closest neighbors,
clusters can “leapfrog” over one another. Thus, it is important to recognize that
the apparent spatial structure of regionalizations will depend on how the 
connectivity of observations is modeled.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Overall, clustering and regionalization are two complementary tools to reduce the
complexity in multivariate data and build better understandings of the spatial structure 
of data. Often, there is simply too much data to examine every variables’ map and its
relation to all other variable maps. 
Thus, clustering reduces this complexity into a single conceptual shorthand by which 
people can easily describe complex and multifaceted data. 
Clustering constructs groups of observations (called <em>clusters</em>)
with coherent <em>profiles</em>, or distinct and internally-consistent 
distributional/descriptive characteristics. 
These profiles are the conceptual shorthand, since members of each cluster should
be more similar to the cluster at large than they are to any other cluster. 
Many different clustering methods exist; they differ on how the “cluster at large” 
is defined, and how “similar” members must be to clusters, or how these clusters
are obtained.
Regionalization is a special kind of clustering with an additional geographic requirement. 
Observations should be grouped so that each spatial cluster,
or <em>region</em>, is spatially-coherent as well as data-coherent. 
Thus, regionalization is often concerned with connectivity in a contiguity 
graph for data collected in areas; this ensures that the regions that are identified
are fully internally-connected. 
However, since many regionalization methods are defined for an arbitrary connectivity structure,
these graphs can be constructed according to different rules as well, such as the k-nearest neighbor graph.</p>

<p>In this chapter, we discussed the conceptual basis for clustering and regionalization, 
as well showing why clustering is done. 
Further, we have shown how to build clusters using spatial data science packages, 
and how to interrogate the meaning of these clusters as well.
More generally, clusters are often used in predictive and explanatory settings, 
in addition to being used for exploratory analysis in their own right.
Clustering and regionalization are intimately related to the analysis of spatial autocorrelation as well,
since the spatial structure and covariation in multivariate spatial data is what
determines the spatial structure and data profile of discovered clusters or regions.
Thus, clustering and regionalization are essential tools for the spatial data scientist.</p>

              <nav class="c-page__nav">
  
    
    <a id="js-page__nav__prev" class="c-page__nav__prev" href="/book/notebooks/09_spatial_inequality">
      〈 <span class="u-margin-right-tiny"></span> Spatial Inequality
    </a>
  

  
    
    <a id="js-page__nav__next" class="c-page__nav__next" href="/book/notebooks/11_regression">
      Regression <span class="u-margin-right-tiny"></span> 〉
    </a>
  
</nav>

            </div>
          </div>
        </div>
      </main>
    </div>

  </body>
</html>
